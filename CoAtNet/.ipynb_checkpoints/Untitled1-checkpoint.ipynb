{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632e8a80-c37f-470a-999f-93117f24af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Resize([224, 224])\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47efae38-8223-49a4-8e3f-d34a127b0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torchsummary import summary\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c406a86e-266e-46d7-a24d-bc1f35103dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample is False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c7c434-7b94-41e7-930f-e39641cc4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample is False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                # down-sample in the first conv\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
    "                          groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else:\n",
    "            return x + self.conv(x)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else:\n",
    "            x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': Transformer}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
    "        self.s1 = self._make_layer(\n",
    "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        self.s2 = self._make_layer(\n",
    "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "        self.s3 = self._make_layer(\n",
    "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        self.s4 = self._make_layer(\n",
    "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s0(x)\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=True))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06e2104-d4ec-4436-925b-4dac17bd94da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]             576\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              GELU-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-5         [-1, 64, 112, 112]             128\n",
      "              GELU-6         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 96, 56, 56]           6,144\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "           Conv2d-10          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-11          [-1, 256, 56, 56]             512\n",
      "             GELU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]           2,304\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             GELU-15          [-1, 256, 56, 56]               0\n",
      "AdaptiveAvgPool2d-16            [-1, 256, 1, 1]               0\n",
      "           Linear-17                   [-1, 16]           4,096\n",
      "             GELU-18                   [-1, 16]               0\n",
      "           Linear-19                  [-1, 256]           4,096\n",
      "          Sigmoid-20                  [-1, 256]               0\n",
      "               SE-21          [-1, 256, 56, 56]               0\n",
      "           Conv2d-22           [-1, 96, 56, 56]          24,576\n",
      "      BatchNorm2d-23           [-1, 96, 56, 56]             192\n",
      "          PreNorm-24           [-1, 96, 56, 56]               0\n",
      "           MBConv-25           [-1, 96, 56, 56]               0\n",
      "      BatchNorm2d-26           [-1, 96, 56, 56]             192\n",
      "           Conv2d-27          [-1, 384, 56, 56]          36,864\n",
      "      BatchNorm2d-28          [-1, 384, 56, 56]             768\n",
      "             GELU-29          [-1, 384, 56, 56]               0\n",
      "           Conv2d-30          [-1, 384, 56, 56]           3,456\n",
      "      BatchNorm2d-31          [-1, 384, 56, 56]             768\n",
      "             GELU-32          [-1, 384, 56, 56]               0\n",
      "AdaptiveAvgPool2d-33            [-1, 384, 1, 1]               0\n",
      "           Linear-34                   [-1, 24]           9,216\n",
      "             GELU-35                   [-1, 24]               0\n",
      "           Linear-36                  [-1, 384]           9,216\n",
      "          Sigmoid-37                  [-1, 384]               0\n",
      "               SE-38          [-1, 384, 56, 56]               0\n",
      "           Conv2d-39           [-1, 96, 56, 56]          36,864\n",
      "      BatchNorm2d-40           [-1, 96, 56, 56]             192\n",
      "          PreNorm-41           [-1, 96, 56, 56]               0\n",
      "           MBConv-42           [-1, 96, 56, 56]               0\n",
      "        MaxPool2d-43           [-1, 96, 28, 28]               0\n",
      "           Conv2d-44          [-1, 192, 28, 28]          18,432\n",
      "      BatchNorm2d-45           [-1, 96, 56, 56]             192\n",
      "           Conv2d-46          [-1, 384, 28, 28]          36,864\n",
      "      BatchNorm2d-47          [-1, 384, 28, 28]             768\n",
      "             GELU-48          [-1, 384, 28, 28]               0\n",
      "           Conv2d-49          [-1, 384, 28, 28]           3,456\n",
      "      BatchNorm2d-50          [-1, 384, 28, 28]             768\n",
      "             GELU-51          [-1, 384, 28, 28]               0\n",
      "AdaptiveAvgPool2d-52            [-1, 384, 1, 1]               0\n",
      "           Linear-53                   [-1, 24]           9,216\n",
      "             GELU-54                   [-1, 24]               0\n",
      "           Linear-55                  [-1, 384]           9,216\n",
      "          Sigmoid-56                  [-1, 384]               0\n",
      "               SE-57          [-1, 384, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 28, 28]          73,728\n",
      "      BatchNorm2d-59          [-1, 192, 28, 28]             384\n",
      "          PreNorm-60          [-1, 192, 28, 28]               0\n",
      "           MBConv-61          [-1, 192, 28, 28]               0\n",
      "      BatchNorm2d-62          [-1, 192, 28, 28]             384\n",
      "           Conv2d-63          [-1, 768, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 768, 28, 28]           1,536\n",
      "             GELU-65          [-1, 768, 28, 28]               0\n",
      "           Conv2d-66          [-1, 768, 28, 28]           6,912\n",
      "      BatchNorm2d-67          [-1, 768, 28, 28]           1,536\n",
      "             GELU-68          [-1, 768, 28, 28]               0\n",
      "AdaptiveAvgPool2d-69            [-1, 768, 1, 1]               0\n",
      "           Linear-70                   [-1, 48]          36,864\n",
      "             GELU-71                   [-1, 48]               0\n",
      "           Linear-72                  [-1, 768]          36,864\n",
      "          Sigmoid-73                  [-1, 768]               0\n",
      "               SE-74          [-1, 768, 28, 28]               0\n",
      "           Conv2d-75          [-1, 192, 28, 28]         147,456\n",
      "      BatchNorm2d-76          [-1, 192, 28, 28]             384\n",
      "          PreNorm-77          [-1, 192, 28, 28]               0\n",
      "           MBConv-78          [-1, 192, 28, 28]               0\n",
      "      BatchNorm2d-79          [-1, 192, 28, 28]             384\n",
      "           Conv2d-80          [-1, 768, 28, 28]         147,456\n",
      "      BatchNorm2d-81          [-1, 768, 28, 28]           1,536\n",
      "             GELU-82          [-1, 768, 28, 28]               0\n",
      "           Conv2d-83          [-1, 768, 28, 28]           6,912\n",
      "      BatchNorm2d-84          [-1, 768, 28, 28]           1,536\n",
      "             GELU-85          [-1, 768, 28, 28]               0\n",
      "AdaptiveAvgPool2d-86            [-1, 768, 1, 1]               0\n",
      "           Linear-87                   [-1, 48]          36,864\n",
      "             GELU-88                   [-1, 48]               0\n",
      "           Linear-89                  [-1, 768]          36,864\n",
      "          Sigmoid-90                  [-1, 768]               0\n",
      "               SE-91          [-1, 768, 28, 28]               0\n",
      "           Conv2d-92          [-1, 192, 28, 28]         147,456\n",
      "      BatchNorm2d-93          [-1, 192, 28, 28]             384\n",
      "          PreNorm-94          [-1, 192, 28, 28]               0\n",
      "           MBConv-95          [-1, 192, 28, 28]               0\n",
      "        MaxPool2d-96          [-1, 192, 14, 14]               0\n",
      "           Conv2d-97          [-1, 384, 14, 14]          73,728\n",
      "        MaxPool2d-98          [-1, 192, 14, 14]               0\n",
      "        Rearrange-99             [-1, 196, 192]               0\n",
      "       LayerNorm-100             [-1, 196, 192]             384\n",
      "          Linear-101             [-1, 196, 768]         147,456\n",
      "         Softmax-102          [-1, 8, 196, 196]               0\n",
      "          Linear-103             [-1, 196, 384]          98,688\n",
      "         Dropout-104             [-1, 196, 384]               0\n",
      "       Attention-105             [-1, 196, 384]               0\n",
      "         PreNorm-106             [-1, 196, 384]               0\n",
      "       Rearrange-107          [-1, 384, 14, 14]               0\n",
      "       Rearrange-108             [-1, 196, 384]               0\n",
      "       LayerNorm-109             [-1, 196, 384]             768\n",
      "          Linear-110             [-1, 196, 768]         295,680\n",
      "            GELU-111             [-1, 196, 768]               0\n",
      "         Dropout-112             [-1, 196, 768]               0\n",
      "          Linear-113             [-1, 196, 384]         295,296\n",
      "         Dropout-114             [-1, 196, 384]               0\n",
      "     FeedForward-115             [-1, 196, 384]               0\n",
      "         PreNorm-116             [-1, 196, 384]               0\n",
      "       Rearrange-117          [-1, 384, 14, 14]               0\n",
      "     Transformer-118          [-1, 384, 14, 14]               0\n",
      "       Rearrange-119             [-1, 196, 384]               0\n",
      "       LayerNorm-120             [-1, 196, 384]             768\n",
      "          Linear-121             [-1, 196, 768]         294,912\n",
      "         Softmax-122          [-1, 8, 196, 196]               0\n",
      "          Linear-123             [-1, 196, 384]          98,688\n",
      "         Dropout-124             [-1, 196, 384]               0\n",
      "       Attention-125             [-1, 196, 384]               0\n",
      "         PreNorm-126             [-1, 196, 384]               0\n",
      "       Rearrange-127          [-1, 384, 14, 14]               0\n",
      "       Rearrange-128             [-1, 196, 384]               0\n",
      "       LayerNorm-129             [-1, 196, 384]             768\n",
      "          Linear-130            [-1, 196, 1536]         591,360\n",
      "            GELU-131            [-1, 196, 1536]               0\n",
      "         Dropout-132            [-1, 196, 1536]               0\n",
      "          Linear-133             [-1, 196, 384]         590,208\n",
      "         Dropout-134             [-1, 196, 384]               0\n",
      "     FeedForward-135             [-1, 196, 384]               0\n",
      "         PreNorm-136             [-1, 196, 384]               0\n",
      "       Rearrange-137          [-1, 384, 14, 14]               0\n",
      "     Transformer-138          [-1, 384, 14, 14]               0\n",
      "       Rearrange-139             [-1, 196, 384]               0\n",
      "       LayerNorm-140             [-1, 196, 384]             768\n",
      "          Linear-141             [-1, 196, 768]         294,912\n",
      "         Softmax-142          [-1, 8, 196, 196]               0\n",
      "          Linear-143             [-1, 196, 384]          98,688\n",
      "         Dropout-144             [-1, 196, 384]               0\n",
      "       Attention-145             [-1, 196, 384]               0\n",
      "         PreNorm-146             [-1, 196, 384]               0\n",
      "       Rearrange-147          [-1, 384, 14, 14]               0\n",
      "       Rearrange-148             [-1, 196, 384]               0\n",
      "       LayerNorm-149             [-1, 196, 384]             768\n",
      "          Linear-150            [-1, 196, 1536]         591,360\n",
      "            GELU-151            [-1, 196, 1536]               0\n",
      "         Dropout-152            [-1, 196, 1536]               0\n",
      "          Linear-153             [-1, 196, 384]         590,208\n",
      "         Dropout-154             [-1, 196, 384]               0\n",
      "     FeedForward-155             [-1, 196, 384]               0\n",
      "         PreNorm-156             [-1, 196, 384]               0\n",
      "       Rearrange-157          [-1, 384, 14, 14]               0\n",
      "     Transformer-158          [-1, 384, 14, 14]               0\n",
      "       Rearrange-159             [-1, 196, 384]               0\n",
      "       LayerNorm-160             [-1, 196, 384]             768\n",
      "          Linear-161             [-1, 196, 768]         294,912\n",
      "         Softmax-162          [-1, 8, 196, 196]               0\n",
      "          Linear-163             [-1, 196, 384]          98,688\n",
      "         Dropout-164             [-1, 196, 384]               0\n",
      "       Attention-165             [-1, 196, 384]               0\n",
      "         PreNorm-166             [-1, 196, 384]               0\n",
      "       Rearrange-167          [-1, 384, 14, 14]               0\n",
      "       Rearrange-168             [-1, 196, 384]               0\n",
      "       LayerNorm-169             [-1, 196, 384]             768\n",
      "          Linear-170            [-1, 196, 1536]         591,360\n",
      "            GELU-171            [-1, 196, 1536]               0\n",
      "         Dropout-172            [-1, 196, 1536]               0\n",
      "          Linear-173             [-1, 196, 384]         590,208\n",
      "         Dropout-174             [-1, 196, 384]               0\n",
      "     FeedForward-175             [-1, 196, 384]               0\n",
      "         PreNorm-176             [-1, 196, 384]               0\n",
      "       Rearrange-177          [-1, 384, 14, 14]               0\n",
      "     Transformer-178          [-1, 384, 14, 14]               0\n",
      "       Rearrange-179             [-1, 196, 384]               0\n",
      "       LayerNorm-180             [-1, 196, 384]             768\n",
      "          Linear-181             [-1, 196, 768]         294,912\n",
      "         Softmax-182          [-1, 8, 196, 196]               0\n",
      "          Linear-183             [-1, 196, 384]          98,688\n",
      "         Dropout-184             [-1, 196, 384]               0\n",
      "       Attention-185             [-1, 196, 384]               0\n",
      "         PreNorm-186             [-1, 196, 384]               0\n",
      "       Rearrange-187          [-1, 384, 14, 14]               0\n",
      "       Rearrange-188             [-1, 196, 384]               0\n",
      "       LayerNorm-189             [-1, 196, 384]             768\n",
      "          Linear-190            [-1, 196, 1536]         591,360\n",
      "            GELU-191            [-1, 196, 1536]               0\n",
      "         Dropout-192            [-1, 196, 1536]               0\n",
      "          Linear-193             [-1, 196, 384]         590,208\n",
      "         Dropout-194             [-1, 196, 384]               0\n",
      "     FeedForward-195             [-1, 196, 384]               0\n",
      "         PreNorm-196             [-1, 196, 384]               0\n",
      "       Rearrange-197          [-1, 384, 14, 14]               0\n",
      "     Transformer-198          [-1, 384, 14, 14]               0\n",
      "       MaxPool2d-199            [-1, 384, 7, 7]               0\n",
      "          Conv2d-200            [-1, 768, 7, 7]         294,912\n",
      "       MaxPool2d-201            [-1, 384, 7, 7]               0\n",
      "       Rearrange-202              [-1, 49, 384]               0\n",
      "       LayerNorm-203              [-1, 49, 384]             768\n",
      "          Linear-204              [-1, 49, 768]         294,912\n",
      "         Softmax-205            [-1, 8, 49, 49]               0\n",
      "          Linear-206              [-1, 49, 768]         197,376\n",
      "         Dropout-207              [-1, 49, 768]               0\n",
      "       Attention-208              [-1, 49, 768]               0\n",
      "         PreNorm-209              [-1, 49, 768]               0\n",
      "       Rearrange-210            [-1, 768, 7, 7]               0\n",
      "       Rearrange-211              [-1, 49, 768]               0\n",
      "       LayerNorm-212              [-1, 49, 768]           1,536\n",
      "          Linear-213             [-1, 49, 1536]       1,181,184\n",
      "            GELU-214             [-1, 49, 1536]               0\n",
      "         Dropout-215             [-1, 49, 1536]               0\n",
      "          Linear-216              [-1, 49, 768]       1,180,416\n",
      "         Dropout-217              [-1, 49, 768]               0\n",
      "     FeedForward-218              [-1, 49, 768]               0\n",
      "         PreNorm-219              [-1, 49, 768]               0\n",
      "       Rearrange-220            [-1, 768, 7, 7]               0\n",
      "     Transformer-221            [-1, 768, 7, 7]               0\n",
      "       Rearrange-222              [-1, 49, 768]               0\n",
      "       LayerNorm-223              [-1, 49, 768]           1,536\n",
      "          Linear-224              [-1, 49, 768]         589,824\n",
      "         Softmax-225            [-1, 8, 49, 49]               0\n",
      "          Linear-226              [-1, 49, 768]         197,376\n",
      "         Dropout-227              [-1, 49, 768]               0\n",
      "       Attention-228              [-1, 49, 768]               0\n",
      "         PreNorm-229              [-1, 49, 768]               0\n",
      "       Rearrange-230            [-1, 768, 7, 7]               0\n",
      "       Rearrange-231              [-1, 49, 768]               0\n",
      "       LayerNorm-232              [-1, 49, 768]           1,536\n",
      "          Linear-233             [-1, 49, 3072]       2,362,368\n",
      "            GELU-234             [-1, 49, 3072]               0\n",
      "         Dropout-235             [-1, 49, 3072]               0\n",
      "          Linear-236              [-1, 49, 768]       2,360,064\n",
      "         Dropout-237              [-1, 49, 768]               0\n",
      "     FeedForward-238              [-1, 49, 768]               0\n",
      "         PreNorm-239              [-1, 49, 768]               0\n",
      "       Rearrange-240            [-1, 768, 7, 7]               0\n",
      "     Transformer-241            [-1, 768, 7, 7]               0\n",
      "       AvgPool2d-242            [-1, 768, 1, 1]               0\n",
      "          Linear-243                   [-1, 10]           7,680\n",
      "================================================================\n",
      "Total params: 16,996,288\n",
      "Trainable params: 16,996,288\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 382.17\n",
      "Params size (MB): 64.84\n",
      "Estimated Total Size (MB): 447.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "model = CoAtNet((224, 224), 1, num_blocks, channels, num_classes=10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model,(1,224,224))      # print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6daaeb1-5a2d-49c4-bdc4-e72c66e81553",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "losses = np.inf\n",
    "path = r'./CoAtNet.pt'\n",
    "dataloader = train_loader\n",
    "def train(model, dataloader, device, optimizer, loss_function, epoch, path):\n",
    "    model.train()\n",
    "    global losses\n",
    "    for batch, datas in enumerate(dataloader):\n",
    "        x,y = datas[0].to(device),datas[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_function(output,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch%5 == 0:\n",
    "            if losses > loss.item():\n",
    "                losses = loss.item()\n",
    "                torch.save(model.state_dict(), path)\n",
    "            print('Train Epoch：{}===[{}/{}=>({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch,batch*len(x),len(dataloader.dataset),100. * batch / len(dataloader),loss.item()))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b625c-752e-4ed7-8df4-ac1041e999ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch：0===[0/60000=>(0%)]\tLoss: 2.372536\n",
      "Train Epoch：0===[80/60000=>(0%)]\tLoss: 2.494418\n",
      "Train Epoch：0===[160/60000=>(0%)]\tLoss: 2.401782\n",
      "Train Epoch：0===[240/60000=>(0%)]\tLoss: 1.959248\n",
      "Train Epoch：0===[320/60000=>(1%)]\tLoss: 2.052110\n",
      "Train Epoch：0===[400/60000=>(1%)]\tLoss: 1.555836\n",
      "Train Epoch：0===[480/60000=>(1%)]\tLoss: 1.969613\n",
      "Train Epoch：0===[560/60000=>(1%)]\tLoss: 1.932822\n",
      "Train Epoch：0===[640/60000=>(1%)]\tLoss: 1.686328\n",
      "Train Epoch：0===[720/60000=>(1%)]\tLoss: 2.082303\n",
      "Train Epoch：0===[800/60000=>(1%)]\tLoss: 1.618774\n",
      "Train Epoch：0===[880/60000=>(1%)]\tLoss: 1.513176\n",
      "Train Epoch：0===[960/60000=>(2%)]\tLoss: 1.282192\n",
      "Train Epoch：0===[1040/60000=>(2%)]\tLoss: 1.193216\n",
      "Train Epoch：0===[1120/60000=>(2%)]\tLoss: 1.053589\n",
      "Train Epoch：0===[1200/60000=>(2%)]\tLoss: 1.299429\n",
      "Train Epoch：0===[1280/60000=>(2%)]\tLoss: 1.229477\n",
      "Train Epoch：0===[1360/60000=>(2%)]\tLoss: 1.778033\n",
      "Train Epoch：0===[1440/60000=>(2%)]\tLoss: 1.204030\n",
      "Train Epoch：0===[1520/60000=>(3%)]\tLoss: 1.120396\n",
      "Train Epoch：0===[1600/60000=>(3%)]\tLoss: 1.286443\n",
      "Train Epoch：0===[1680/60000=>(3%)]\tLoss: 0.611736\n",
      "Train Epoch：0===[1760/60000=>(3%)]\tLoss: 0.505504\n",
      "Train Epoch：0===[1840/60000=>(3%)]\tLoss: 0.846263\n",
      "Train Epoch：0===[1920/60000=>(3%)]\tLoss: 1.275025\n",
      "Train Epoch：0===[2000/60000=>(3%)]\tLoss: 0.987086\n",
      "Train Epoch：0===[2080/60000=>(3%)]\tLoss: 0.782139\n",
      "Train Epoch：0===[2160/60000=>(4%)]\tLoss: 0.650116\n",
      "Train Epoch：0===[2240/60000=>(4%)]\tLoss: 0.899485\n",
      "Train Epoch：0===[2320/60000=>(4%)]\tLoss: 0.870173\n",
      "Train Epoch：0===[2400/60000=>(4%)]\tLoss: 1.448120\n",
      "Train Epoch：0===[2480/60000=>(4%)]\tLoss: 0.814123\n",
      "Train Epoch：0===[2560/60000=>(4%)]\tLoss: 0.655110\n",
      "Train Epoch：0===[2640/60000=>(4%)]\tLoss: 0.493745\n",
      "Train Epoch：0===[2720/60000=>(5%)]\tLoss: 0.814771\n",
      "Train Epoch：0===[2800/60000=>(5%)]\tLoss: 0.693782\n",
      "Train Epoch：0===[2880/60000=>(5%)]\tLoss: 0.531246\n",
      "Train Epoch：0===[2960/60000=>(5%)]\tLoss: 0.501961\n",
      "Train Epoch：0===[3040/60000=>(5%)]\tLoss: 0.470524\n",
      "Train Epoch：0===[3120/60000=>(5%)]\tLoss: 0.485948\n",
      "Train Epoch：0===[3200/60000=>(5%)]\tLoss: 0.423867\n",
      "Train Epoch：0===[3280/60000=>(5%)]\tLoss: 0.431440\n",
      "Train Epoch：0===[3360/60000=>(6%)]\tLoss: 0.597195\n",
      "Train Epoch：0===[3440/60000=>(6%)]\tLoss: 0.533132\n",
      "Train Epoch：0===[3520/60000=>(6%)]\tLoss: 1.100727\n",
      "Train Epoch：0===[3600/60000=>(6%)]\tLoss: 0.140023\n",
      "Train Epoch：0===[3680/60000=>(6%)]\tLoss: 1.582586\n",
      "Train Epoch：0===[3760/60000=>(6%)]\tLoss: 0.697667\n",
      "Train Epoch：0===[3840/60000=>(6%)]\tLoss: 0.295184\n",
      "Train Epoch：0===[3920/60000=>(7%)]\tLoss: 0.630629\n",
      "Train Epoch：0===[4000/60000=>(7%)]\tLoss: 0.347136\n",
      "Train Epoch：0===[4080/60000=>(7%)]\tLoss: 0.289640\n",
      "Train Epoch：0===[4160/60000=>(7%)]\tLoss: 0.523988\n",
      "Train Epoch：0===[4240/60000=>(7%)]\tLoss: 0.412279\n",
      "Train Epoch：0===[4320/60000=>(7%)]\tLoss: 0.788751\n",
      "Train Epoch：0===[4400/60000=>(7%)]\tLoss: 0.419287\n",
      "Train Epoch：0===[4480/60000=>(7%)]\tLoss: 0.605343\n",
      "Train Epoch：0===[4560/60000=>(8%)]\tLoss: 0.949553\n",
      "Train Epoch：0===[4640/60000=>(8%)]\tLoss: 0.851374\n",
      "Train Epoch：0===[4720/60000=>(8%)]\tLoss: 0.370397\n",
      "Train Epoch：0===[4800/60000=>(8%)]\tLoss: 0.403075\n",
      "Train Epoch：0===[4880/60000=>(8%)]\tLoss: 0.362001\n",
      "Train Epoch：0===[4960/60000=>(8%)]\tLoss: 0.479304\n",
      "Train Epoch：0===[5040/60000=>(8%)]\tLoss: 0.674823\n",
      "Train Epoch：0===[5120/60000=>(9%)]\tLoss: 0.951710\n",
      "Train Epoch：0===[5200/60000=>(9%)]\tLoss: 0.685078\n",
      "Train Epoch：0===[5280/60000=>(9%)]\tLoss: 0.722966\n",
      "Train Epoch：0===[5360/60000=>(9%)]\tLoss: 0.853720\n",
      "Train Epoch：0===[5440/60000=>(9%)]\tLoss: 0.379886\n",
      "Train Epoch：0===[5520/60000=>(9%)]\tLoss: 0.318582\n",
      "Train Epoch：0===[5600/60000=>(9%)]\tLoss: 0.414900\n",
      "Train Epoch：0===[5680/60000=>(9%)]\tLoss: 0.394716\n",
      "Train Epoch：0===[5760/60000=>(10%)]\tLoss: 0.288967\n",
      "Train Epoch：0===[5840/60000=>(10%)]\tLoss: 1.040060\n",
      "Train Epoch：0===[5920/60000=>(10%)]\tLoss: 0.440881\n",
      "Train Epoch：0===[6000/60000=>(10%)]\tLoss: 0.239904\n",
      "Train Epoch：0===[6080/60000=>(10%)]\tLoss: 0.276908\n",
      "Train Epoch：0===[6160/60000=>(10%)]\tLoss: 0.153324\n",
      "Train Epoch：0===[6240/60000=>(10%)]\tLoss: 0.545035\n",
      "Train Epoch：0===[6320/60000=>(11%)]\tLoss: 0.599761\n",
      "Train Epoch：0===[6400/60000=>(11%)]\tLoss: 0.310905\n",
      "Train Epoch：0===[6480/60000=>(11%)]\tLoss: 0.324210\n",
      "Train Epoch：0===[6560/60000=>(11%)]\tLoss: 0.113643\n",
      "Train Epoch：0===[6640/60000=>(11%)]\tLoss: 0.457822\n",
      "Train Epoch：0===[6720/60000=>(11%)]\tLoss: 0.629952\n",
      "Train Epoch：0===[6800/60000=>(11%)]\tLoss: 0.274628\n",
      "Train Epoch：0===[6880/60000=>(11%)]\tLoss: 0.274383\n",
      "Train Epoch：0===[6960/60000=>(12%)]\tLoss: 0.892139\n",
      "Train Epoch：0===[7040/60000=>(12%)]\tLoss: 0.269492\n",
      "Train Epoch：0===[7120/60000=>(12%)]\tLoss: 0.120266\n",
      "Train Epoch：0===[7200/60000=>(12%)]\tLoss: 0.532675\n",
      "Train Epoch：0===[7280/60000=>(12%)]\tLoss: 0.401001\n",
      "Train Epoch：0===[7360/60000=>(12%)]\tLoss: 0.246860\n",
      "Train Epoch：0===[7440/60000=>(12%)]\tLoss: 0.618311\n",
      "Train Epoch：0===[7520/60000=>(13%)]\tLoss: 0.267872\n",
      "Train Epoch：0===[7600/60000=>(13%)]\tLoss: 0.397032\n",
      "Train Epoch：0===[7680/60000=>(13%)]\tLoss: 0.448917\n",
      "Train Epoch：0===[7760/60000=>(13%)]\tLoss: 0.890486\n",
      "Train Epoch：0===[7840/60000=>(13%)]\tLoss: 0.488569\n",
      "Train Epoch：0===[7920/60000=>(13%)]\tLoss: 0.547509\n",
      "Train Epoch：0===[8000/60000=>(13%)]\tLoss: 0.519687\n",
      "Train Epoch：0===[8080/60000=>(13%)]\tLoss: 0.349950\n",
      "Train Epoch：0===[8160/60000=>(14%)]\tLoss: 0.117954\n",
      "Train Epoch：0===[8240/60000=>(14%)]\tLoss: 0.224722\n",
      "Train Epoch：0===[8320/60000=>(14%)]\tLoss: 0.489063\n",
      "Train Epoch：0===[8400/60000=>(14%)]\tLoss: 0.265306\n",
      "Train Epoch：0===[8480/60000=>(14%)]\tLoss: 0.744957\n",
      "Train Epoch：0===[8560/60000=>(14%)]\tLoss: 0.068556\n",
      "Train Epoch：0===[8640/60000=>(14%)]\tLoss: 0.220803\n",
      "Train Epoch：0===[8720/60000=>(15%)]\tLoss: 0.445231\n",
      "Train Epoch：0===[8800/60000=>(15%)]\tLoss: 0.564517\n",
      "Train Epoch：0===[8880/60000=>(15%)]\tLoss: 0.331601\n",
      "Train Epoch：0===[8960/60000=>(15%)]\tLoss: 0.333668\n",
      "Train Epoch：0===[9040/60000=>(15%)]\tLoss: 0.454895\n",
      "Train Epoch：0===[9120/60000=>(15%)]\tLoss: 0.091407\n",
      "Train Epoch：0===[9200/60000=>(15%)]\tLoss: 0.304768\n",
      "Train Epoch：0===[9280/60000=>(15%)]\tLoss: 0.266004\n",
      "Train Epoch：0===[9360/60000=>(16%)]\tLoss: 0.393223\n",
      "Train Epoch：0===[9440/60000=>(16%)]\tLoss: 0.456016\n",
      "Train Epoch：0===[9520/60000=>(16%)]\tLoss: 0.324180\n",
      "Train Epoch：0===[9600/60000=>(16%)]\tLoss: 0.366276\n",
      "Train Epoch：0===[9680/60000=>(16%)]\tLoss: 0.229551\n",
      "Train Epoch：0===[9760/60000=>(16%)]\tLoss: 0.513659\n",
      "Train Epoch：0===[9840/60000=>(16%)]\tLoss: 0.310397\n",
      "Train Epoch：0===[9920/60000=>(17%)]\tLoss: 0.163574\n",
      "Train Epoch：0===[10000/60000=>(17%)]\tLoss: 0.161846\n",
      "Train Epoch：0===[10080/60000=>(17%)]\tLoss: 0.112787\n",
      "Train Epoch：0===[10160/60000=>(17%)]\tLoss: 0.268511\n",
      "Train Epoch：0===[10240/60000=>(17%)]\tLoss: 0.147894\n",
      "Train Epoch：0===[10320/60000=>(17%)]\tLoss: 0.059558\n",
      "Train Epoch：0===[10400/60000=>(17%)]\tLoss: 0.081122\n",
      "Train Epoch：0===[10480/60000=>(17%)]\tLoss: 0.205768\n",
      "Train Epoch：0===[10560/60000=>(18%)]\tLoss: 0.127128\n",
      "Train Epoch：0===[10640/60000=>(18%)]\tLoss: 0.071034\n",
      "Train Epoch：0===[10720/60000=>(18%)]\tLoss: 0.048009\n",
      "Train Epoch：0===[10800/60000=>(18%)]\tLoss: 0.253682\n",
      "Train Epoch：0===[10880/60000=>(18%)]\tLoss: 0.170439\n",
      "Train Epoch：0===[10960/60000=>(18%)]\tLoss: 0.090602\n",
      "Train Epoch：0===[11040/60000=>(18%)]\tLoss: 0.125613\n",
      "Train Epoch：0===[11120/60000=>(19%)]\tLoss: 0.495828\n",
      "Train Epoch：0===[11200/60000=>(19%)]\tLoss: 0.891855\n",
      "Train Epoch：0===[11280/60000=>(19%)]\tLoss: 0.334063\n",
      "Train Epoch：0===[11360/60000=>(19%)]\tLoss: 0.091291\n",
      "Train Epoch：0===[11440/60000=>(19%)]\tLoss: 0.059668\n",
      "Train Epoch：0===[11520/60000=>(19%)]\tLoss: 0.086129\n",
      "Train Epoch：0===[11600/60000=>(19%)]\tLoss: 0.247487\n",
      "Train Epoch：0===[11680/60000=>(19%)]\tLoss: 0.392784\n",
      "Train Epoch：0===[11760/60000=>(20%)]\tLoss: 0.125403\n",
      "Train Epoch：0===[11840/60000=>(20%)]\tLoss: 0.221344\n",
      "Train Epoch：0===[11920/60000=>(20%)]\tLoss: 0.017438\n",
      "Train Epoch：0===[12000/60000=>(20%)]\tLoss: 0.275532\n",
      "Train Epoch：0===[12080/60000=>(20%)]\tLoss: 0.280935\n",
      "Train Epoch：0===[12160/60000=>(20%)]\tLoss: 0.330251\n",
      "Train Epoch：0===[12240/60000=>(20%)]\tLoss: 0.067526\n",
      "Train Epoch：0===[12320/60000=>(21%)]\tLoss: 0.093554\n",
      "Train Epoch：0===[12400/60000=>(21%)]\tLoss: 0.146959\n",
      "Train Epoch：0===[12480/60000=>(21%)]\tLoss: 0.047919\n",
      "Train Epoch：0===[12560/60000=>(21%)]\tLoss: 0.116754\n",
      "Train Epoch：0===[12640/60000=>(21%)]\tLoss: 0.798758\n",
      "Train Epoch：0===[12720/60000=>(21%)]\tLoss: 0.289154\n",
      "Train Epoch：0===[12800/60000=>(21%)]\tLoss: 0.087004\n",
      "Train Epoch：0===[12880/60000=>(21%)]\tLoss: 0.138337\n",
      "Train Epoch：0===[12960/60000=>(22%)]\tLoss: 0.361646\n",
      "Train Epoch：0===[13040/60000=>(22%)]\tLoss: 0.410672\n",
      "Train Epoch：0===[13120/60000=>(22%)]\tLoss: 0.330073\n",
      "Train Epoch：0===[13200/60000=>(22%)]\tLoss: 0.047564\n",
      "Train Epoch：0===[13280/60000=>(22%)]\tLoss: 0.068643\n",
      "Train Epoch：0===[13360/60000=>(22%)]\tLoss: 0.287921\n",
      "Train Epoch：0===[13440/60000=>(22%)]\tLoss: 0.385275\n",
      "Train Epoch：0===[13520/60000=>(23%)]\tLoss: 0.235962\n",
      "Train Epoch：0===[13600/60000=>(23%)]\tLoss: 0.076150\n",
      "Train Epoch：0===[13680/60000=>(23%)]\tLoss: 0.090937\n",
      "Train Epoch：0===[13760/60000=>(23%)]\tLoss: 0.234663\n",
      "Train Epoch：0===[13840/60000=>(23%)]\tLoss: 0.061228\n",
      "Train Epoch：0===[13920/60000=>(23%)]\tLoss: 0.519835\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(model, dataloader, device, optimizer, loss_function, epoch, path)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905cbe5-e04e-4cef-bf45-5f57d4c6debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoAtNet((224, 224), 1, num_blocks, channels, num_classes=10)\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for batch, datas in enumerate(test_loader):\n",
    "        x,y = datas[0].to(device),datas[1].to(device)\n",
    "        output = model(x)\n",
    "        pre = output.max(1, keepdim=True)[1]\n",
    "        correct += pre.eq(y.view_as(pre)).sum().item()\n",
    "print(correct/(len(test_loader)*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e2c50-28a0-4eef-b59c-29253df0c0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_gpu]",
   "language": "python",
   "name": "conda-env-torch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
